#!/bin/sh
#############################
#  Copyright notice
#  (c) 2008-2016 Chi Hoang 
#############################

# router
#echo 1 > /proc/sys/net/ipv4/ip_forward
net.ipv4.ip_forward=1
#
# Anzahl Testpakete, die ausgesendet werden, um festzustellen,
# ob der Partner aktiv ist. Danach gilt die Verbindung als unterbrochen
#echo 3 > /proc/sys/net/ipv4/tcp_keepalive_probes
net.ipv4.tcp_keepalive_probes=3
#
# whatever
#echo 1 > /proc/sys/net/ipv4/tcp_tw_recycle
net.ipv4.tcp_tw_recycle=1
#
# unterbreche Verbindungsaufbau nach 3 SYN-Paketen
# Default ist 6
#echo 2 > /proc/sys/net/ipv4/tcp_syn_retries
net.ipv4.tcp_syn_retries=2
#
# unterbreche Verbindungsaufbau nach 3 SYN/ACK-Paketen
# Default ist 6
#echo 2 > /proc/sys/net/ipv4/tcp_synack_retries
net.ipv4.tcp_synack_retries=2
#
# SYN attack
#echo 1 > /proc/sys/net/core/tcp_syncookies
#echo 2048 > tcp_max_syn_backlog
#echo 2 > tcp_synack_retries 
#
# Anzahl Versuche, eine ICMP-Nachricht zuzustellen
#echo 3 > /proc/sys/net/ipv4/route/redirect_number
net.ipv4.route.redirect_number=3
#
# default:512
#echo 1000 > /proc/sys/net/ipv4/route/gc_thresh
net.ipv4.route.gc_thresh=1000
#
# Network Subsystem Memory Management
#
# Enable really big (>65kB) TCP window scaling if we want it.
#echo 1 > /proc/sys/net/ipv4/tcp_window_scaling
net.ipv4.tcp_window_scaling=1
#
# Turn on the tcp_sack
#echo 1 > /proc/sys/net/ipv4/tcp_sack
net.ipv4.tcp_sack=1
#
# tcp_fack should be on because of sack
#echo 1 > /proc/sys/net/ipv4/tcp_fack
net.ipv4.tcp_fack=1
#
# Sometimes, packet reordering in a network can be interpreted as packet loss and 
# hence increasing the value of this parameter should improve performance (default is “3?
# Set TCP Re-Ordering value in kernel
# default:3
#echo 48 > /proc/sys/net/ipv4/tcp_reordering
net.ipv4.tcp_reordering=48
#
# Enables/Disables the behaviour of cachek performance charecteristics connection. 
# By default, Linux Kernel remembers connection performance and congestion charecteristics.
# default = 0
#echo 1 > /proc/sys/net/ipv4/tcp_no_metrics_save
net.ipv4.tcp_no_metrics_save=1
#
# You can set this to one of the manu available high speed congestion variants like “cubic” 
# “hs-tcp” (default is “reno”)
# default = "reno"
#echo "cubic" > /proc/sys/net/ipv4/tcp_congestion_control
net.ipv4.tcp_congestion_control=cubic
#
# ipfrag_high_thresh setzt das Limit für den maximalen Speicher,
# der für das fragmentieren von Paketen reserviert wird. Wenn mehr Speicher
# benötigt wird fängt der Kernel an fragmentierte Pakete zu
# verwerfen. Dieses Limit ni sehr kritisch für Denial of Service Angriffe,
# da es, wenn es zu niedrig gewählt wird zu viele Pakete verwirft, und wenn
# es zu hoch gewählt wird zu viel Speicher, und CPU Leistung verbrauchen
# kann. ipfrag_low_thresh ist für das minimale Limit zuständig.
#
# default high: 262144 (256kb)
# default low:  196608 (192kb)
#
# einfach mal 300% mehr nehmen
#high=262144
#low=196608
#let high=400*$high/100
#let low=400*$low/100
#high=1048576
#low=786432
#echo $high > /proc/sys/net/ipv4/ipfrag_high_thresh
net.ipv4.ipfrag_high_thresh=1048576
#echo $low > /proc/sys/net/ipv4/ipfrag_low_thresh
net.ipv4.ipfrag_low_thresh=786432
#
# Länge der Warteschlange für eingehende TCP-Verbindungsanforderungen für
# einen Socket (default 300)
#echo 3000 > /proc/sys/net/ipv4/tcp_max_syn_backlog
net.ipv4.tcp_max_syn_backlog=3000
#
#
# add more conntrack
# default 16384
#echo 25000 > /proc/sys/net/nf_conntrack_maunge der Warteschlange der routing tabelle (experimental)
# default:8192 (gc_thresh x 16)
#echo 10000 > /proc/sys/net/ipv4/route/max_size
net.ipv4.route.max_size=10000
#
# TCP Autotuning setting. "The tcp_mem variable defines how the TCP stack should behave when it comes to memory usage. ...
# The first value specified in the tcp_mem variable tells the kernel the low threshold. Below this point, the TCP stack do
# not bother at all about putting any pressure on the memory usage by different TCP sockets. ... The second value tells the
# kernel at which point to start pressuring memory usage down. ... The final value tells the kernel how many memory pages it
# may use maximally. If this value is reached, TCP streams and packets start getting dropped until we u a lower memory
# usage again. This value includes all TCP sockets currently in use."
#
# min:      65535  (64kb)
# default: 131071 (128kb)
# test:    262144 (256kb)
#
#tmem=262144
#let tmem_max=400*$tmem/100
#tmem_max=1048 4096 $tmem $tmem_max > /proc/sys/net/ipv4/tcp_mem
net.ipv4.tcp_mem=1024 4096 262144 1048576
# TCP Autotuning setting. "The first value tells the kernel the minimum receive buffer for each TCP connection, and this
# buffer is always allocated to a TCP socket, even under high pressure on the system. ... The second value specified tells
# the kernel the default receive buffer allocated for each TCP socket. This value overrides the /proc/sys/net/core/rmem_default
# value used by other protocols. ... The third and last value specified in this variable specifies the maximum receive buffer
# that can be allocated for a TCP socket."
#
# Increase the maximum and default receive socket buffer size
# (Increase default size for 4 Mbytes/s)
# default: 131071 (128kb)
# test:    262144 (256kb)
#
#rmem=262144
#let rmem_max=400*$rmem/100
#rmem_max=1048576
#echo $rmem > /proc/sys/net/core/rmem_default
net.core.rmem_default=262144
#echo $rmem_max > /proc/sys/net/core/rmem_max
net.core.rmem_max=1048576
#echo 4096 $rmem $rmem_max > /proc/sys/net/ipv4/tcp_rmem
net.ipv4.tcp_rmem=4096 262144 1048576
#
# TCP Autotuning setting. "This variable takes 3 different values which holds information on how much TCP sendbuffer memory
# space each TCP socket has to use. Every TCP socket has this much buffer space to use before the buffer is filled up. Each of
# the three values are used under different conditions. ... The first value in this variable tells the minimum TCP send buffer
# space available for a single TCP socket. ... The second value in the variable tells us the default buffer space allowed for a
# single TCP socket to use. ... The third value tells the kernel the maximum TCP send buffer space."
# min:      65535  (64kb)
# default: 131071 (128kb)
# test:    262144 (256kb)
#
#wmem=262144
#let wmem_max=400*$wmem/100
#wmem_max=1048576
#echo $wmem > /proc/sys/net/core/wmem_default
net.core.wmem_default=262144
#echo $wmem_max > /proc/sys/net/core/wmem_max
net.core.wmem_max=1048576
#echo 4096 $wmem $wmem_max > /proc/sys/net/ipv4/tcp_wmem
net.ipv4.tcp_wmem=4096 262144 1048576
#
# Maximum ancillary buffer size allowed per socket. Ancillary data is a 
# sequence of struct cmsghdr structures with appended data. 
# The default size is 10240 bytes
#optmem_max=10240
#let max=400*$optmem_max/100
#max=40960
#echo $max > /proc/sys/net/core/optmem_max
net.core.optmem_max=10240
#
# default: 10
#echo 64 > /proc/sys/net/unix/max_dgram_qlen
net.unix.max_dgram_qlen=64
#
# default: 50
#echo 128 > /proc/sys/net/core/message_burst
net.core.message_burst=128
#
# default: 290
#echo 512 > /proc/sys/net/core/mod_cong
net.core.mod_cong=512
#
# default: 100
#echo 128 > /proc/sys/net/core/lo_cong
net.core.lo_cong=128
#
# default: 20
#echo 64 > /proc/sys/net/core/no_cong
net.core.no_cong=64
#
# default: 20
#echo 64 > /proc/sys/net/core/no_cong_thresh
net.core.no_cong_thresh=64
#Listed below are general net settings, like "netdev_max_backlog" (typically 300), the length of all your
# network packets. This value can limit your network bandwidth when receiving packets, Linux has to wait
# up to scheduling time to flush buffers (due to bottom half mechanism), about 1000/HZ ms
#
#   300    *        100             =     30 000
# packets     HZ(Timeslice freq)         packets/s
#
# 30 000   *       1000             =      30 M
# packets     average (Bytes/packet)   throughput Bytes/s
#
# If you want to get higher throughput, you need to increase netdev_max_backlog, by typing:
# echo 4000 > /proc/sys/net/core/netdev_max_backlog
#
# default value: 300
#echo 3048 > /proc/sys/net/core/netdev_max_backlog
net.core.netdev_max_backlog=3000
#
#
#
# Set timeout on kernel panics (auto reboots after # seconds):
# default:0 (no autoreboot)
## 
#echo 60 > /proc/sys/kernel/panic
#
# less shmmax
#kernel.shmmax = 268435456
# default: 33554432/1024/1024 = 32 MB,
# this machine has only 16 MB so what the fuck is this!!!
# recommendation is 50% of machine memory
#shmmax=268435456
#let shmmax=25*$shmmax/100
#echo $shmmax > /proc/sys/kernel/shmmax
kernel.shmmax=134217728
# PAGE_SIZE: now 4096 (see patches-Archives) (default 16384)
# wenn Bytes, dann gleich SHMMAX; wenn Seiten, dann ceil(SHMMAX/PAGE_SIZE)
# ich nehme mal ceil(SHMMAX/PAGE_SIZE)
# default: 2097152
# mplayer bug shmall must be greater 32678
#echo 32678 > /proc/sys/kernel/shmall
kernel.shmall=32678
#
# default: 4096
#echo 128 > /proc/sys/kernel/shmmni
kernel.shmmni=128
#
#
# Kernel-Message-Queue
#
# default: 8192
#echo 2048 > /proc/sys/kernel/msgmax
kernel.msgmax=2000
#
# default: 16384
#echo 2048 > /proc/sys/kernel/msgmnb
#kernel.msgmnb=2000
#
#
# This will enusre that immediatly subsequent connections use these values.
#echo 1 > /proc/sys/net/ipv4/route/flush
net.ipv4.route.flush=1
#
#
#